\subsection{Forward and backward propagation}
Forward and backward propagation are used to compute $\nabla_\theta J(\theta)$ for a given $\theta$. This is needed by the optimization algorithms used to adjust the parameters of the network to minimize $J$.

Note that $J$ may be not differentiable: if this is the case, for the sake of simplicity, the very same notation $\nabla_\theta J(\theta)$ will denote any of the subgradients of $J(\theta)$ with respect to $\theta$, and we will continue to refer to it as to the gradient of the objective function, without loss of generality.

In algorithm \ref{alg:prop} we present the pseudocode of forward and backward propagation for a single sample; for a detailed description of these algorithms refer to \ref{bibitem:goodfellow}.

\begin{algorithm}[htbp]
    \caption{Forward and Backward Propagation\\
        \textbf{Requires:} the input data $\x$, the target output $\y$.
    }
    \label{alg:prop}
    \begin{algorithmic}
        \State $\o_0 = \x$
        \For{$l = 1, \ldots, L$} \Comment {Forward propagates the input}
            \State $\h_l = \W_l\o_{l-1} + \b_l$
            \State $\o_l = f_l(\h_l)$
        \EndFor
        \State $\yh = \o_L$
        \State $J = \L (\y, \yh) + \lambda\Omega(\theta)$ \Comment {Computes the loss} \\
        \State $\g \gets \nabla_{\yh} J = \nabla_{\yh} \L(\y, \yh)$ \Comment {Computes the gradient of the loss}
        \For{$l = L, \ldots, 1$} \Comment {Backward propagates the gradient}
            \State $\g \gets \nabla_{\h_l} J = \g \odot f'(\h_l)$
            \State $\nabla_{\b_l} J = \g + \lambda \nabla_{\b_l}\Omega(\theta)$
            \State $\nabla_{\W_l} J = \g \o_{l-1}^T + \lambda \nabla_{\W_l}\Omega(\theta)$
            \State $\g \gets \nabla_{\o_{l-1}} J = \W_l^T \g$
        \EndFor
    \end{algorithmic}
\end{algorithm}

\subsection{Heavy Ball Gradient (HBG)}
The Heavy Ball Gradient (HBG) is a deflected gradient optimization method widely used in neural networks training; its pseudocode is presented in algorithm \ref{alg:HBGD}.

The algorithm iteratively updates the parameters of the network until a stopping condition is satisfied. The update direction at epoch $i$ is a linear combination of the negative gradient direction at that epoch with the update direction at epoch $i - 1$, where the gradient is computed by forward and backward propagation. The coefficients of the linear combination are the hyperparameters $\alpha$ and $\beta$, which are called respectively learning rate and momentum.

As stopping condition we chose the reaching of a max number of epochs, given by $\texttt{MAX\_EPOCHS}$. This is very typical in Machine Learning, but many other stopping conditions are possible: for instance, the algorithm may be designed to stop as soon as the gradient norm drops below a certain threshold.

\begin{algorithm}[htbp]
    \caption{Heavy Ball Gradient (HBG)\\
        \textbf{Requires:} the function $J(\theta)$ to optimize, the starting point $\theta_0$, the learning rate hyperparameter $\alpha$, the momentum hyperparameter $\beta$, the max number of epochs $\texttt{MAX\_EPOCHS}$.
    }
    \label{alg:HBGD}
    \begin{algorithmic}
        \State $\theta \gets \theta_0$
        \For{$i = 1, \ldots, \texttt{MAX\_EPOCHS}$}
            \If{$i = 1$}
                \State $\Delta \theta \gets -\alpha \nabla_\theta J(\theta)$
            \Else
                \State $\Delta \theta \gets -\alpha \nabla_\theta J(\theta) + \beta \d$
            \EndIf
            \State $\theta \gets \theta + \Delta \theta$
            \State $\d \gets \Delta \theta$
        \EndFor
    \end{algorithmic}
\end{algorithm}

\subsection{Camerini-Fratta-Maffioli subgradient (CFM)}
Camerini-Fratta-Maffioli (CFM) is a deflected subgradient optimization method; its pseudocode is presented in algorithm \ref{alg:CFM}. For a wide introduction to subgradient methods refer to \ref{bibitem:boyd}; for an in-depth analysis of deflected subgradient methods refer to \ref{bibitem:frangioni}; for a detailed description of this specific algorithm refer to \ref{bibitem:cfm}.

\begin{algorithm}[htbp]
    \caption{Camerini-Fratta-Maffioli (CFM) with exact Polyak step size\\
        \textbf{Requires:} the function $J(\theta)$ to optimize, the starting point $\theta_0$, the momentum hyperparameter $\gamma$, the optimal value $J^*$ of $J(\theta)$ (or an estimate of it), the max number of epochs $\texttt{MAX\_EPOCHS}$.
    }
    \label{alg:CFM}
    \begin{algorithmic}
        \State $\theta \gets \theta_0$
        \For{$i = 1, \ldots, \texttt{MAX\_EPOCHS}$}
            \State $\g \gets \nabla_\theta J(\theta)$
            \If {$i = 1$}
                \State $\d \gets \g$
            \Else
                \State $\beta \gets \text{max}\left(0, -\gamma\frac{\d^T\g}{\Vert\d\Vert_2^2}\right)$
                \State $\d \gets \g+\beta\d$
            \EndIf
            \State $\alpha \gets \frac{J(\theta) - J^*}{\Vert\d\Vert_2^2}$
            \State $\theta \gets \theta - \alpha\d$
        \EndFor
    \end{algorithmic}
\end{algorithm}

The algorithm iteratively updates the parameters of the network until a stopping condition is satisfied, exactly like HBG. The update direction at each epoch $i$ depends not only on the negative gradient direction at that epoch, but also on the update direction at epoch $i - 1$. The gradient is still computed by forward and backward propagation, and the momentum hyperparameter is $\gamma \in [0,2]$.

The step size $\alpha$ along the update direction is called Polyak step size\footnote{Many other step sizes are worthy of consideration when considering deflected subgradient methods. For a detailed description of the alternatives refer to \ref{bibitem:frangioni}.}, and is a function of $J^*$, the optimal value of the objective function. In general, this is hardly available at optimization time, so an estimate of it may be needed.

In algorithm \ref{alg:CFM-TL}, a common technique called target following is used to provide a moving estimate of $J^*$. Here, $J^*$ is estimated by the target level $J_{\text{best}}^{(i)} - \delta^{(i)}$, where the reference value $J_{\text{best}}^{(i)}$ is the best objective value up to epoch $i$, and the threshold $\delta^{(i)}$ can be interpreted as an estimate of how suboptimal the current value is.

\begin{algorithm}[htbp]
    \caption{Camerini-Fratta-Maffioli (CFM) with target level\\
        \textbf{Requires:} the function $J(\theta)$ to optimize, the starting point $\theta_0$, the momentum hyperparameter $\gamma$, the target level $\{\delta^{(i)}\}$, the max number of epochs $\texttt{MAX\_EPOCHS}$.
    }
    \label{alg:CFM-TL}
    \begin{algorithmic}
        \State $\theta \gets \theta_0$
        \State $J_{\text{best}} \gets J(\theta)$
        \For{$i = 1, \ldots, \texttt{MAX\_EPOCHS}$}
            \State $\g \gets \nabla_\theta J(\theta)$
            \If {$i = 1$}
                \State $\d \gets \g$
            \Else
                \State $\beta \gets \text{max}\left(0, -\gamma\frac{\d^T\g}{\Vert\d\Vert_2^2}\right)$
                \State $\d \gets \g+\beta\d$
            \EndIf
            \State $\alpha \gets \frac{J(\theta) - J_{\text{best}} + \delta^{(i)}}{\Vert\d\Vert_2^2}$
            \State $\theta \gets \theta - \alpha\d$
            \State $J_{\text{best}} \gets \text{min}\left(J_{\text{best}}, J(\theta)\right)$
        \EndFor
    \end{algorithmic}
\end{algorithm}